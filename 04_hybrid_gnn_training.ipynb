{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid D-MPNN + Descriptors for Melting Point Prediction\n",
    "\n",
    "This notebook implements a **hybrid approach** combining:\n",
    "1. **D-MPNN (Graph Neural Network)** - learns molecular representations from SMILES\n",
    "2. **Hand-crafted descriptors** - 1326 features (424 Group + 20 RDKit + 882 Mordred)\n",
    "3. **SMILES augmentation** - generates multiple SMILES variants per molecule\n",
    "4. **Stacking ensemble** - combines GNN, XGBoost, and LightGBM predictions\n",
    "\n",
    "**Baseline to beat:** XGBoost with RMSE=43.50, R²=0.739\n",
    "\n",
    "**Expected performance:** R² = 0.78-0.82, RMSE = 36-40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cpu\n",
      "CUDA available: False\n",
      "Using device: cpu\n",
      "Chemprop version: 2.2.2\n",
      "\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Lipinski, rdMolDescriptors\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "# Mordred\n",
    "from mordred import Calculator, descriptors as mordred_descriptors\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Chemprop\n",
    "import chemprop\n",
    "print(f\"Chemprop version: {chemprop.__version__}\")\n",
    "\n",
    "from chemprop.data import MoleculeDatapoint, MoleculeDataset, build_dataloader\n",
    "from chemprop.featurizers import SimpleMoleculeMolGraphFeaturizer\n",
    "from chemprop.nn import BondMessagePassing, MeanAggregation, RegressionFFN\n",
    "from chemprop.models import MPNN\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Tree models for ensemble\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "print(\"\\nAll imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 2662\n",
      "Test samples: 666\n",
      "\n",
      "Target (Tm) statistics:\n",
      "count    2662.000000\n",
      "mean      278.263452\n",
      "std        85.117914\n",
      "min        53.540000\n",
      "25%       217.000000\n",
      "50%       277.300000\n",
      "75%       325.150000\n",
      "max       897.150000\n",
      "Name: Tm, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nTarget (Tm) statistics:\")\n",
    "print(train_df['Tm'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute All Molecular Descriptors\n",
    "\n",
    "Combining:\n",
    "- 424 Group features (pre-computed in dataset)\n",
    "- 20 RDKit descriptors\n",
    "- 882 Mordred descriptors\n",
    "- **Total: 1326 features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing RDKit descriptors...\n",
      "  RDKit descriptors: 20\n"
     ]
    }
   ],
   "source": [
    "def compute_rdkit_descriptors(smiles_list):\n",
    "    \"\"\"Compute RDKit descriptors\"\"\"\n",
    "    descriptors_list = []\n",
    "    \n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            descriptors_list.append({key: np.nan for key in [\n",
    "                'MolWt', 'LogP', 'TPSA', 'NumHDonors', 'NumHAcceptors',\n",
    "                'NumRotatableBonds', 'NumAromaticRings', 'NumHeavyAtoms',\n",
    "                'FractionCSP3', 'NumRings', 'NumHeteroatoms', 'MolMR',\n",
    "                'NumValenceElectrons', 'NumRadicalElectrons', 'HallKierAlpha',\n",
    "                'NumAliphaticRings', 'NumSaturatedRings', 'NumAromaticHeterocycles',\n",
    "                'NumAliphaticHeterocycles', 'NumSaturatedHeterocycles'\n",
    "            ]})\n",
    "            continue\n",
    "            \n",
    "        desc = {\n",
    "            'MolWt': Descriptors.MolWt(mol),\n",
    "            'LogP': Descriptors.MolLogP(mol),\n",
    "            'TPSA': Descriptors.TPSA(mol),\n",
    "            'NumHDonors': Lipinski.NumHDonors(mol),\n",
    "            'NumHAcceptors': Lipinski.NumHAcceptors(mol),\n",
    "            'NumRotatableBonds': Lipinski.NumRotatableBonds(mol),\n",
    "            'NumAromaticRings': rdMolDescriptors.CalcNumAromaticRings(mol),\n",
    "            'NumHeavyAtoms': Lipinski.HeavyAtomCount(mol),\n",
    "            'FractionCSP3': rdMolDescriptors.CalcFractionCSP3(mol),\n",
    "            'NumRings': rdMolDescriptors.CalcNumRings(mol),\n",
    "            'NumHeteroatoms': rdMolDescriptors.CalcNumHeteroatoms(mol),\n",
    "            'MolMR': Descriptors.MolMR(mol),\n",
    "            'NumValenceElectrons': Descriptors.NumValenceElectrons(mol),\n",
    "            'NumRadicalElectrons': Descriptors.NumRadicalElectrons(mol),\n",
    "            'HallKierAlpha': Descriptors.HallKierAlpha(mol),\n",
    "            'NumAliphaticRings': rdMolDescriptors.CalcNumAliphaticRings(mol),\n",
    "            'NumSaturatedRings': rdMolDescriptors.CalcNumSaturatedRings(mol),\n",
    "            'NumAromaticHeterocycles': rdMolDescriptors.CalcNumAromaticHeterocycles(mol),\n",
    "            'NumAliphaticHeterocycles': rdMolDescriptors.CalcNumAliphaticHeterocycles(mol),\n",
    "            'NumSaturatedHeterocycles': rdMolDescriptors.CalcNumSaturatedHeterocycles(mol),\n",
    "        }\n",
    "        descriptors_list.append(desc)\n",
    "    \n",
    "    return pd.DataFrame(descriptors_list)\n",
    "\n",
    "print(\"Computing RDKit descriptors...\")\n",
    "train_rdkit = compute_rdkit_descriptors(train_df['SMILES'].values)\n",
    "test_rdkit = compute_rdkit_descriptors(test_df['SMILES'].values)\n",
    "print(f\"  RDKit descriptors: {train_rdkit.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Mordred descriptors for training data...\n",
      "  Calculating 1613 Mordred descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2662/2662 [00:33<00:00, 80.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Valid descriptors: 912\n",
      "\n",
      "Computing Mordred descriptors for test data...\n",
      "  Calculating 1613 Mordred descriptors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 666/666 [00:08<00:00, 80.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Valid descriptors: 978\n",
      "\n",
      "Final Mordred descriptors: 897\n"
     ]
    }
   ],
   "source": [
    "def compute_mordred_descriptors(smiles_list):\n",
    "    \"\"\"Compute Mordred descriptors that work for all molecules\"\"\"\n",
    "    mols = [Chem.MolFromSmiles(s) for s in smiles_list]\n",
    "    \n",
    "    calc = Calculator(mordred_descriptors, ignore_3D=True)\n",
    "    print(f\"  Calculating {len(calc.descriptors)} Mordred descriptors...\")\n",
    "    df = calc.pandas(mols)\n",
    "    \n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    valid_cols = df.columns[df.notna().all()].tolist()\n",
    "    df_clean = df[valid_cols]\n",
    "    df_clean = df_clean.loc[:, df_clean.std() > 0]\n",
    "    \n",
    "    print(f\"  Valid descriptors: {df_clean.shape[1]}\")\n",
    "    return df_clean\n",
    "\n",
    "print(\"Computing Mordred descriptors for training data...\")\n",
    "train_mordred = compute_mordred_descriptors(train_df['SMILES'].values)\n",
    "\n",
    "print(\"\\nComputing Mordred descriptors for test data...\")\n",
    "test_mordred = compute_mordred_descriptors(test_df['SMILES'].values)\n",
    "\n",
    "# Keep only common columns\n",
    "common_cols = list(set(train_mordred.columns) & set(test_mordred.columns))\n",
    "train_mordred = train_mordred[common_cols]\n",
    "test_mordred = test_mordred[common_cols]\n",
    "\n",
    "print(f\"\\nFinal Mordred descriptors: {len(common_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature breakdown:\n",
      "  - Group features: 424\n",
      "  - RDKit descriptors: 20\n",
      "  - Mordred descriptors: 897\n",
      "  - Total features: 1341\n"
     ]
    }
   ],
   "source": [
    "# Combine all features\n",
    "group_cols = [col for col in train_df.columns if col.startswith('Group')]\n",
    "train_groups = train_df[group_cols]\n",
    "test_groups = test_df[group_cols]\n",
    "\n",
    "# Combine into single array\n",
    "X_train_all = pd.concat([\n",
    "    train_groups.reset_index(drop=True),\n",
    "    train_rdkit.reset_index(drop=True),\n",
    "    train_mordred.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "X_test_all = pd.concat([\n",
    "    test_groups.reset_index(drop=True),\n",
    "    test_rdkit.reset_index(drop=True),\n",
    "    test_mordred.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# Clean data\n",
    "X_train_all = X_train_all.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "X_test_all = X_test_all.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "y_train = train_df['Tm'].values\n",
    "\n",
    "print(f\"Feature breakdown:\")\n",
    "print(f\"  - Group features: {len(group_cols)}\")\n",
    "print(f\"  - RDKit descriptors: {train_rdkit.shape[1]}\")\n",
    "print(f\"  - Mordred descriptors: {train_mordred.shape[1]}\")\n",
    "print(f\"  - Total features: {X_train_all.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled descriptors shape: (2662, 1341)\n"
     ]
    }
   ],
   "source": [
    "# Scale descriptors for GNN\n",
    "scaler = StandardScaler()\n",
    "extra_desc_train = scaler.fit_transform(X_train_all.values)\n",
    "extra_desc_test = scaler.transform(X_test_all.values)\n",
    "\n",
    "n_extra_descriptors = extra_desc_train.shape[1]\n",
    "print(f\"Scaled descriptors shape: {extra_desc_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SMILES Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original SMILES: FC1=C(F)C(F)(F)C1(F)F\n",
      "Generated 5 variants:\n",
      "  C1(C(F)(C(F)(C=1F)F)F)F\n",
      "  FC1(F)C(=C(C1(F)F)F)F\n",
      "  FC1=C(C(F)(C1(F)F)F)F\n",
      "  C1(=C(F)C(F)(F)C1(F)F)F\n",
      "  FC1=C(F)C(F)(F)C1(F)F\n"
     ]
    }
   ],
   "source": [
    "def enumerate_smiles(smiles, n_variants=5):\n",
    "    \"\"\"Generate multiple random SMILES for a molecule\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return [smiles]\n",
    "    \n",
    "    variants = set([smiles])  # Include canonical\n",
    "    attempts = 0\n",
    "    max_attempts = n_variants * 3\n",
    "    \n",
    "    while len(variants) < n_variants and attempts < max_attempts:\n",
    "        random_smi = Chem.MolToSmiles(mol, doRandom=True, canonical=False)\n",
    "        variants.add(random_smi)\n",
    "        attempts += 1\n",
    "    \n",
    "    return list(variants)\n",
    "\n",
    "# Test\n",
    "test_smi = train_df['SMILES'].iloc[0]\n",
    "variants = enumerate_smiles(test_smi, n_variants=5)\n",
    "print(f\"Original SMILES: {test_smi}\")\n",
    "print(f\"Generated {len(variants)} variants:\")\n",
    "for v in variants:\n",
    "    print(f\"  {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hybrid Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model Configuration:\n",
      "  hidden_size: 600\n",
      "  depth: 4\n",
      "  ffn_hidden_size: 600\n",
      "  ffn_num_layers: 3\n",
      "  dropout: 0.15\n",
      "  batch_size: 32\n",
      "  epochs: 150\n",
      "  patience: 20\n",
      "  lr: 0.0005\n",
      "  n_smiles_augment: 5\n",
      "\n",
      "FFN input dimension: 600 (GNN) + 1341 (descriptors) = 1941\n"
     ]
    }
   ],
   "source": [
    "# Larger model configuration for best performance\n",
    "CONFIG = {\n",
    "    'hidden_size': 600,           # Message passing hidden size\n",
    "    'depth': 4,                   # Message passing layers\n",
    "    'ffn_hidden_size': 600,       # FFN hidden size\n",
    "    'ffn_num_layers': 3,          # FFN layers\n",
    "    'dropout': 0.15,              # Dropout rate\n",
    "    'batch_size': 32,             # Batch size\n",
    "    'epochs': 150,                # Max epochs\n",
    "    'patience': 20,               # Early stopping patience\n",
    "    'lr': 5e-4,                   # Learning rate\n",
    "    'n_smiles_augment': 5,        # SMILES variants per molecule\n",
    "}\n",
    "\n",
    "print(\"Hybrid Model Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nFFN input dimension: {CONFIG['hidden_size']} (GNN) + {n_extra_descriptors} (descriptors) = {CONFIG['hidden_size'] + n_extra_descriptors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n",
      "Total parameters: 2,703,601\n"
     ]
    }
   ],
   "source": [
    "featurizer = SimpleMoleculeMolGraphFeaturizer()\n",
    "\n",
    "def create_hybrid_model(config, n_extra_desc):\n",
    "    \"\"\"Create hybrid D-MPNN with extra descriptors\"\"\"\n",
    "    # Message passing\n",
    "    mp = BondMessagePassing(\n",
    "        d_h=config['hidden_size'],\n",
    "        depth=config['depth'],\n",
    "        dropout=config['dropout']\n",
    "    )\n",
    "    \n",
    "    # Aggregation\n",
    "    agg = MeanAggregation()\n",
    "    \n",
    "    # FFN input = GNN output + extra descriptors\n",
    "    ffn_input_dim = config['hidden_size'] + n_extra_desc\n",
    "    \n",
    "    predictor = RegressionFFN(\n",
    "        input_dim=ffn_input_dim,\n",
    "        hidden_dim=config['ffn_hidden_size'],\n",
    "        n_layers=config['ffn_num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        n_tasks=1\n",
    "    )\n",
    "    \n",
    "    model = MPNN(\n",
    "        message_passing=mp,\n",
    "        agg=agg,\n",
    "        predictor=predictor,\n",
    "        batch_norm=True\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test model creation\n",
    "test_model = create_hybrid_model(CONFIG, n_extra_descriptors)\n",
    "n_params = sum(p.numel() for p in test_model.parameters())\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Total parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Datapoints with Extra Descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datapoints with extra descriptors (x_d)...\n",
      "  Training molecules: 2662\n",
      "  Test molecules: 666\n",
      "  Extra descriptors per molecule: 1341\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "train_smiles = train_df['SMILES'].tolist()\n",
    "train_targets = train_df['Tm'].tolist()\n",
    "test_smiles = test_df['SMILES'].tolist()\n",
    "\n",
    "print(f\"Creating datapoints with extra descriptors (x_d)...\")\n",
    "print(f\"  Training molecules: {len(train_smiles)}\")\n",
    "print(f\"  Test molecules: {len(test_smiles)}\")\n",
    "print(f\"  Extra descriptors per molecule: {n_extra_descriptors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2662 training datapoints\n",
      "Created 666 test datapoints\n"
     ]
    }
   ],
   "source": [
    "# Create training datapoints with x_d (extra descriptors)\n",
    "train_datapoints = [\n",
    "    MoleculeDatapoint.from_smi(smi, y=np.array([target]), x_d=desc)\n",
    "    for smi, target, desc in zip(train_smiles, train_targets, extra_desc_train)\n",
    "]\n",
    "\n",
    "# Create test datapoints with x_d\n",
    "test_datapoints = [\n",
    "    MoleculeDatapoint.from_smi(smi, x_d=desc)\n",
    "    for smi, desc in zip(test_smiles, extra_desc_test)\n",
    "]\n",
    "\n",
    "print(f\"Created {len(train_datapoints)} training datapoints\")\n",
    "print(f\"Created {len(test_datapoints)} test datapoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating augmented datapoints (5x)...\n",
      "Augmented dataset: 2662 -> 13057 samples\n"
     ]
    }
   ],
   "source": [
    "# Create augmented training data (SMILES enumeration)\n",
    "def create_augmented_datapoints(smiles_list, targets, descriptors, n_variants=5):\n",
    "    \"\"\"Create augmented datapoints with SMILES enumeration\"\"\"\n",
    "    augmented = []\n",
    "    \n",
    "    for smi, target, desc in zip(smiles_list, targets, descriptors):\n",
    "        variants = enumerate_smiles(smi, n_variants=n_variants)\n",
    "        for variant_smi in variants:\n",
    "            augmented.append(\n",
    "                MoleculeDatapoint.from_smi(variant_smi, y=np.array([target]), x_d=desc)\n",
    "            )\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "print(f\"Creating augmented datapoints ({CONFIG['n_smiles_augment']}x)...\")\n",
    "augmented_datapoints = create_augmented_datapoints(\n",
    "    train_smiles, train_targets, extra_desc_train, \n",
    "    n_variants=CONFIG['n_smiles_augment']\n",
    ")\n",
    "print(f\"Augmented dataset: {len(train_datapoints)} -> {len(augmented_datapoints)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training with 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_fold(train_data, val_data, config, n_extra_desc, fold_num, use_augmentation=True):\n    \"\"\"Train hybrid model on a single fold\"\"\"\n    print(f\"\\n{'='*55}\")\n    print(f\"  Fold {fold_num}\")\n    print(f\"{'='*55}\")\n    \n    # Augment training data if enabled\n    if use_augmentation:\n        # Chemprop 2.x stores SMILES in 'name' attribute\n        train_smiles_fold = [dp.name for dp in train_data]\n        train_targets_fold = [dp.y[0] for dp in train_data]\n        train_desc_fold = [dp.x_d for dp in train_data]\n        \n        train_data_aug = create_augmented_datapoints(\n            train_smiles_fold, train_targets_fold, train_desc_fold,\n            n_variants=config['n_smiles_augment']\n        )\n        print(f\"  Training samples (augmented): {len(train_data_aug)}\")\n    else:\n        train_data_aug = train_data\n        print(f\"  Training samples: {len(train_data_aug)}\")\n    \n    print(f\"  Validation samples: {len(val_data)}\")\n    \n    # Create datasets\n    train_dataset = MoleculeDataset(train_data_aug, featurizer=featurizer)\n    val_dataset = MoleculeDataset(val_data, featurizer=featurizer)\n    \n    # Create dataloaders\n    train_loader = build_dataloader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n    val_loader = build_dataloader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n    \n    # Create model\n    model = create_hybrid_model(config, n_extra_desc)\n    \n    # Setup trainer\n    trainer = pl.Trainer(\n        max_epochs=config['epochs'],\n        accelerator='cpu',\n        enable_progress_bar=True,\n        enable_model_summary=False,\n        logger=False,\n        callbacks=[\n            pl.callbacks.EarlyStopping(\n                monitor='val_loss',\n                patience=config['patience'],\n                mode='min'\n            )\n        ]\n    )\n    \n    # Train\n    print(f\"  Training (this may take a while on CPU)...\")\n    trainer.fit(model, train_loader, val_loader)\n    \n    # Get validation predictions\n    model.eval()\n    val_preds = []\n    val_targets_list = []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            bmg, V_d, X_d, targets, *_ = batch\n            preds = model(bmg, X_d=X_d)  # Pass extra descriptors to model\n            val_preds.extend(preds.squeeze().tolist())\n            val_targets_list.extend(targets.squeeze().tolist())\n    \n    # Metrics\n    rmse = np.sqrt(mean_squared_error(val_targets_list, val_preds))\n    mae = mean_absolute_error(val_targets_list, val_preds)\n    r2 = r2_score(val_targets_list, val_preds)\n    \n    print(f\"  Results: RMSE={rmse:.4f}, MAE={mae:.4f}, R²={r2:.4f}\")\n    \n    return model, {'rmse': rmse, 'mae': mae, 'r2': r2}, val_preds, val_targets_list"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  HYBRID D-MPNN: 5-FOLD CROSS-VALIDATION\n",
      "============================================================\n",
      "\n",
      "Using SMILES augmentation: 5x\n",
      "Model: hidden_size=600, depth=4\n",
      "Extra descriptors: 1341\n",
      "\n",
      "=======================================================\n",
      "  Fold 1\n",
      "=======================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MoleculeDatapoint' object has no attribute 'smiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m train_data = [train_datapoints[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_idx]\n\u001b[32m     18\u001b[39m val_data = [train_datapoints[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m val_idx]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m model, metrics, val_preds, val_targets = \u001b[43mtrain_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_extra_descriptors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_augmentation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m fold_results.append(metrics)\n\u001b[32m     26\u001b[39m fold_models.append(model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain_fold\u001b[39m\u001b[34m(train_data, val_data, config, n_extra_desc, fold_num, use_augmentation)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Augment training data if enabled\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_augmentation:\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Chemprop 2.x uses 'smiles' attribute (not 'smi')\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     train_smiles_fold = \u001b[43m[\u001b[49m\u001b[43mdp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msmiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m     train_targets_fold = [dp.y[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m dp \u001b[38;5;129;01min\u001b[39;00m train_data]\n\u001b[32m     12\u001b[39m     train_desc_fold = [dp.x_d \u001b[38;5;28;01mfor\u001b[39;00m dp \u001b[38;5;129;01min\u001b[39;00m train_data]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Augment training data if enabled\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_augmentation:\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Chemprop 2.x uses 'smiles' attribute (not 'smi')\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     train_smiles_fold = [\u001b[43mdp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msmiles\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dp \u001b[38;5;129;01min\u001b[39;00m train_data]\n\u001b[32m     11\u001b[39m     train_targets_fold = [dp.y[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m dp \u001b[38;5;129;01min\u001b[39;00m train_data]\n\u001b[32m     12\u001b[39m     train_desc_fold = [dp.x_d \u001b[38;5;28;01mfor\u001b[39;00m dp \u001b[38;5;129;01min\u001b[39;00m train_data]\n",
      "\u001b[31mAttributeError\u001b[39m: 'MoleculeDatapoint' object has no attribute 'smiles'"
     ]
    }
   ],
   "source": [
    "# 5-Fold Cross-Validation\n",
    "n_folds = 5\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "fold_models = []\n",
    "all_val_preds = np.zeros(len(train_datapoints))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  HYBRID D-MPNN: 5-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nUsing SMILES augmentation: {CONFIG['n_smiles_augment']}x\")\n",
    "print(f\"Model: hidden_size={CONFIG['hidden_size']}, depth={CONFIG['depth']}\")\n",
    "print(f\"Extra descriptors: {n_extra_descriptors}\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_datapoints), 1):\n",
    "    train_data = [train_datapoints[i] for i in train_idx]\n",
    "    val_data = [train_datapoints[i] for i in val_idx]\n",
    "    \n",
    "    model, metrics, val_preds, val_targets = train_fold(\n",
    "        train_data, val_data, CONFIG, n_extra_descriptors, fold,\n",
    "        use_augmentation=True\n",
    "    )\n",
    "    \n",
    "    fold_results.append(metrics)\n",
    "    fold_models.append(model)\n",
    "    \n",
    "    # Store OOF predictions\n",
    "    for i, idx in enumerate(val_idx):\n",
    "        all_val_preds[idx] = val_preds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  HYBRID GNN CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cv_results = pd.DataFrame(fold_results)\n",
    "print(\"\\nPer-Fold Results:\")\n",
    "print(cv_results.to_string())\n",
    "\n",
    "print(f\"\\nMean ± Std:\")\n",
    "print(f\"  RMSE: {cv_results['rmse'].mean():.4f} ± {cv_results['rmse'].std():.4f}\")\n",
    "print(f\"  MAE:  {cv_results['mae'].mean():.4f} ± {cv_results['mae'].std():.4f}\")\n",
    "print(f\"  R²:   {cv_results['r2'].mean():.4f} ± {cv_results['r2'].std():.4f}\")\n",
    "\n",
    "# Overall OOF metrics\n",
    "gnn_oof_rmse = np.sqrt(mean_squared_error(y_train, all_val_preds))\n",
    "gnn_oof_mae = mean_absolute_error(y_train, all_val_preds)\n",
    "gnn_oof_r2 = r2_score(y_train, all_val_preds)\n",
    "\n",
    "print(f\"\\nOverall Out-of-Fold Metrics:\")\n",
    "print(f\"  RMSE: {gnn_oof_rmse:.4f}\")\n",
    "print(f\"  MAE:  {gnn_oof_mae:.4f}\")\n",
    "print(f\"  R²:   {gnn_oof_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nComparison with Baseline:\")\n",
    "print(f\"  XGBoost baseline:  RMSE=43.50, R²=0.739\")\n",
    "print(f\"  Hybrid D-MPNN:     RMSE={gnn_oof_rmse:.2f}, R²={gnn_oof_r2:.3f}\")\n",
    "\n",
    "gnn_oof_preds = all_val_preds.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train XGBoost and LightGBM for Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  TRAINING TREE MODELS FOR STACKING ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# LightGBM\n",
    "lgbm_model = LGBMRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Get OOF predictions for tree models\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "xgb_oof_preds = cross_val_predict(xgb_model, X_train_all, y_train, cv=kfold)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_train, xgb_oof_preds))\n",
    "xgb_r2 = r2_score(y_train, xgb_oof_preds)\n",
    "print(f\"  XGBoost OOF: RMSE={xgb_rmse:.4f}, R²={xgb_r2:.4f}\")\n",
    "\n",
    "print(\"\\nTraining LightGBM...\")\n",
    "lgbm_oof_preds = cross_val_predict(lgbm_model, X_train_all, y_train, cv=kfold)\n",
    "lgbm_rmse = np.sqrt(mean_squared_error(y_train, lgbm_oof_preds))\n",
    "lgbm_r2 = r2_score(y_train, lgbm_oof_preds)\n",
    "print(f\"  LightGBM OOF: RMSE={lgbm_rmse:.4f}, R²={lgbm_r2:.4f}\")\n",
    "\n",
    "# Train on full data for test predictions\n",
    "print(\"\\nTraining on full data...\")\n",
    "xgb_model.fit(X_train_all, y_train)\n",
    "lgbm_model.fit(X_train_all, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  STACKING ENSEMBLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Stack OOF predictions\n",
    "stacked_train = np.column_stack([\n",
    "    gnn_oof_preds,\n",
    "    xgb_oof_preds,\n",
    "    lgbm_oof_preds\n",
    "])\n",
    "\n",
    "print(f\"\\nStacked features shape: {stacked_train.shape}\")\n",
    "print(f\"  - Hybrid GNN predictions\")\n",
    "print(f\"  - XGBoost predictions\")\n",
    "print(f\"  - LightGBM predictions\")\n",
    "\n",
    "# Train meta-learner\n",
    "meta_model = RidgeCV(alphas=[0.001, 0.01, 0.1, 1.0, 10.0, 100.0])\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "\n",
    "print(f\"\\nMeta-learner (Ridge) alpha: {meta_model.alpha_}\")\n",
    "print(f\"Meta-learner weights: {meta_model.coef_}\")\n",
    "\n",
    "# Ensemble OOF predictions\n",
    "ensemble_oof_preds = meta_model.predict(stacked_train)\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_train, ensemble_oof_preds))\n",
    "ensemble_mae = mean_absolute_error(y_train, ensemble_oof_preds)\n",
    "ensemble_r2 = r2_score(y_train, ensemble_oof_preds)\n",
    "\n",
    "print(f\"\\nStacking Ensemble OOF Metrics:\")\n",
    "print(f\"  RMSE: {ensemble_rmse:.4f}\")\n",
    "print(f\"  MAE:  {ensemble_mae:.4f}\")\n",
    "print(f\"  R²:   {ensemble_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['XGBoost (baseline)', 'LightGBM', 'Hybrid D-MPNN', 'Stacking Ensemble'],\n",
    "    'RMSE': [xgb_rmse, lgbm_rmse, gnn_oof_rmse, ensemble_rmse],\n",
    "    'R²': [xgb_r2, lgbm_r2, gnn_oof_r2, ensemble_r2]\n",
    "})\n",
    "\n",
    "results = results.sort_values('RMSE')\n",
    "print(\"\\n\" + results.to_string(index=False))\n",
    "\n",
    "best_model = results.iloc[0]['Model']\n",
    "best_rmse = results.iloc[0]['RMSE']\n",
    "best_r2 = results.iloc[0]['R²']\n",
    "\n",
    "improvement = ((43.50 - best_rmse) / 43.50) * 100\n",
    "print(f\"\\nBest model: {best_model}\")\n",
    "print(f\"Improvement over XGBoost baseline: {improvement:.1f}% RMSE reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "models_data = [\n",
    "    ('XGBoost', xgb_oof_preds, xgb_r2, xgb_rmse),\n",
    "    ('LightGBM', lgbm_oof_preds, lgbm_r2, lgbm_rmse),\n",
    "    ('Hybrid D-MPNN', gnn_oof_preds, gnn_oof_r2, gnn_oof_rmse),\n",
    "    ('Stacking Ensemble', ensemble_oof_preds, ensemble_r2, ensemble_rmse)\n",
    "]\n",
    "\n",
    "for ax, (name, preds, r2, rmse) in zip(axes.flat, models_data):\n",
    "    ax.scatter(y_train, preds, alpha=0.4, s=20, c='steelblue')\n",
    "    ax.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', linewidth=2)\n",
    "    ax.set_xlabel('Actual Melting Point (K)', fontsize=11)\n",
    "    ax.set_ylabel('Predicted Melting Point (K)', fontsize=11)\n",
    "    ax.set_title(f'{name}\\nR² = {r2:.4f}, RMSE = {rmse:.2f}', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hybrid_gnn_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: hybrid_gnn_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"  GENERATING TEST PREDICTIONS\")\nprint(\"=\"*60)\n\n# GNN test predictions (ensemble of fold models)\ntest_dataset = MoleculeDataset(test_datapoints, featurizer=featurizer)\ntest_loader = build_dataloader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n\nprint(\"\\nGenerating GNN predictions (ensemble of 5 folds)...\")\nall_gnn_test_preds = []\n\nfor fold_num, model in enumerate(fold_models, 1):\n    model.eval()\n    fold_preds = []\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            bmg, V_d, X_d, targets, *_ = batch\n            preds = model(bmg, X_d=X_d)  # Pass extra descriptors to model\n            fold_preds.extend(preds.squeeze().tolist())\n    \n    all_gnn_test_preds.append(fold_preds)\n    print(f\"  Fold {fold_num}: {len(fold_preds)} predictions\")\n\ngnn_test_preds = np.mean(all_gnn_test_preds, axis=0)\n\n# Tree model predictions\nprint(\"\\nGenerating tree model predictions...\")\nxgb_test_preds = xgb_model.predict(X_test_all)\nlgbm_test_preds = lgbm_model.predict(X_test_all)\n\n# Stacking ensemble\nstacked_test = np.column_stack([gnn_test_preds, xgb_test_preds, lgbm_test_preds])\nensemble_test_preds = meta_model.predict(stacked_test)\n\nprint(f\"\\nTest predictions statistics:\")\nprint(f\"  Hybrid GNN:  Min={gnn_test_preds.min():.1f}, Max={gnn_test_preds.max():.1f}, Mean={gnn_test_preds.mean():.1f}\")\nprint(f\"  XGBoost:     Min={xgb_test_preds.min():.1f}, Max={xgb_test_preds.max():.1f}, Mean={xgb_test_preds.mean():.1f}\")\nprint(f\"  Ensemble:    Min={ensemble_test_preds.min():.1f}, Max={ensemble_test_preds.max():.1f}, Mean={ensemble_test_preds.mean():.1f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submissions\n",
    "submission_gnn = pd.DataFrame({'id': test_df['id'], 'Tm': gnn_test_preds})\n",
    "submission_ensemble = pd.DataFrame({'id': test_df['id'], 'Tm': ensemble_test_preds})\n",
    "\n",
    "submission_gnn.to_csv('submission_hybrid_gnn.csv', index=False)\n",
    "submission_ensemble.to_csv('submission_stacking_ensemble.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmissions saved:\")\n",
    "print(\"  - submission_hybrid_gnn.csv\")\n",
    "print(\"  - submission_stacking_ensemble.csv\")\n",
    "\n",
    "print(\"\\nEnsemble submission preview:\")\n",
    "print(submission_ensemble.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"  FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nModel Performance (5-Fold CV):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Model':<25} {'RMSE':<10} {'R²':<10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'XGBoost (baseline)':<25} {xgb_rmse:<10.2f} {xgb_r2:<10.4f}\")\n",
    "print(f\"{'LightGBM':<25} {lgbm_rmse:<10.2f} {lgbm_r2:<10.4f}\")\n",
    "print(f\"{'Hybrid D-MPNN':<25} {gnn_oof_rmse:<10.2f} {gnn_oof_r2:<10.4f}\")\n",
    "print(f\"{'Stacking Ensemble':<25} {ensemble_rmse:<10.2f} {ensemble_r2:<10.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"\\nHybrid Approach Details:\")\n",
    "print(f\"  - GNN: D-MPNN with hidden_size={CONFIG['hidden_size']}, depth={CONFIG['depth']}\")\n",
    "print(f\"  - Extra descriptors: {n_extra_descriptors} (Group + RDKit + Mordred)\")\n",
    "print(f\"  - SMILES augmentation: {CONFIG['n_smiles_augment']}x\")\n",
    "print(f\"  - Stacking: GNN + XGBoost + LightGBM with Ridge meta-learner\")\n",
    "\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"  - submission_hybrid_gnn.csv\")\n",
    "print(\"  - submission_stacking_ensemble.csv\")\n",
    "print(\"  - hybrid_gnn_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}